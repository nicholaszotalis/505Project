{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8d3ab388-9e4c-4c4e-93b3-2b5a7d294f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e0d3ebc1-fdf6-45f2-9aa5-221e03f8c7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"Train.csv\")\n",
    "train_data = train[\"Text\"]\n",
    "train_labels = train[\"Label\"]\n",
    "\n",
    "labels = train_labels.unique()\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    train_labels[train_labels == labels[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1882aaf1-7339-407c-a56f-0543bc597df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47556\n",
      " Mwangonde: Khansala wachinyamata Akamati achinyamata ndi atsogoleri a mawa, ambiri amaganiza kuti izi ndi nkhambakamwa chabe. Koma achinyamata ena, monga Lusubilo Mwangonde, akukwaniritsa akupherezetsa mawuwa osati pongolota kuti adzakhala, koma kutsogolera kumene chifukwa nthawi yawo yakwana. DAILES BANDA adacheza ndi Mwangonde, khansala wachinyama, yemwe akuimira Jumbo Ward mumzinda wa Mzuzu, motere:  Chisale watuluka nkumangidwanso  Sipakala waimitsa Nyumba ya Malamulo  Pa Wenela pasintha zedi Ali ndi masomphenya: Mwangonde Tikudziweni  Ndine Lusubilo Mwangonde, ndili ndi zaka 27 zakubadwa. Ndinabadwa mbanja la ana asanu ndipo ndine wachinayi kubadwa. Ndimachokera mmudzi mwa Mwamalopa, kwa Paramount Chief Kyungu mboma la Karonga. Sindili pabanja pakadalipano.\n",
      " Mbiri ya maphunziro anu ndi yotani? Maphunziro anga a pulaimale ndidachitira kusukula yapulaiveti ya Viphya mumzinda wa Mzuzu ndipo asekondale ndidachitira pa Phwezi Boys mboma la Rumphi. Ndili ndi diploma ya Accounting ndipo pakadalipano ndikupanga digiri komanso Chartered Accounting kusukulu ya Malawi College of Accountancy (MCA).\n",
      " Mudayamba bwanji zandale? Kuyambira ndili wachichepere, zaka 12, ndakhala ndikukhala mumaudindo a utgogoleri. Ichi ndi china mwa zinthu zomwe zidandilimbikitsa kuti ndikhoza kudzapambana pazisankho. Koma chachikulu chomwe chidandichititsa kuti ndilowe ukhansala chidali chifukwa chakuti ndinkafuna kupereka mpata kwa anthu kuti azitha kuyankhula zakukhosi kwawo polimbikitsa demokalase ndi chitukuko.\n",
      " Ntchito mukugwira ndi zomwe munkayembekezera? Eya, ndiponso ndinkayembekezera zambiri.\n",
      " Masomphenya anu ndi otani pandale? Ine ndine munthu wokhulupirira Mulungu ndipo ndili ndi chikhulupiriro choti Iye ndi amene adzandionetsere zomwe ndikuyera kuchita ndi tsogolo langa.\n",
      " Zinthu zina zomwe mumachita ndi chiyani pambali pa ukhansala? Ndikakhala sindikugwira ntchito yaukhansala ndimakhala ndikuchita bizinesi, nthawi zina ndimakhala ndili kusukulu komwe ndikuchita maphuro anga a digiri. Kuonjezera pamenepo ndili ndi bungwe lomwe ndidayambitsa ndi anzanga ena la Centre for Participatory Democracy lomwe limalimbikitsa demokalase.\n",
      " Zomwe mwakwanitsa ndi zotani? Ndathandiza kuti ntchito yopala misewu ya kudera la Moyale itheke. Misewuyi yakhala nthawi yaitali osapalidwa. Ndidathandiziranso kuti ochita malonda ayambe kumanga mashopu anjerwa ndi kusiya kumangira matabwa kapena zigwagwa. Ndidakwanitsanso kukaimirira khonsolo ya Mzuzu ku Nyumba ya Malamulo. Ndaonanso kuti ntchitoyi yandithandiza kusintha momwe ndimaonera zinthu komanso ndimakumana ndi anthu osiyanasiyana omwe amandiphunzitsa zinthu zambiri.\n",
      "\n",
      "[97, 3629, 5454, 23, 19138, 8, 646, 7, 1236, 280, 4818, 183, 584, 4, 166, 4, 5454, 1405, 8, 989, 3630, 4, 28, 47, 50, 8, 1406, 538, 84, 19139, 32, 43, 69, 612, 4819, 1749, 30, 2395, 97, 4, 7429, 1505, 210, 3, 341, 2, 19140, 307, 312, 2, 1635, 612, 676, 12182, 256, 84, 6347, 11, 291, 121, 123, 126, 134, 135, 52, 5, 36, 71, 90, 92, 647, 620, 65, 42, 7, 677, 12, 1585, 3370, 2977, 8, 3371, 1406, 538, 84, 29, 1948, 32, 43, 402, 975, 603, 488, 976, 5, 253, 1949, 8, 253, 34, 1, 584, 16, 14, 185, 3939, 9214, 39, 445, 115, 500, 9215, 8, 19141, 387, 152, 7430, 6348, 4349, 522, 3157, 21, 19142, 584, 12, 185, 584, 422, 12183, 128, 8, 648, 34, 528, 24, 34, 1, 166, 4, 5454, 19143, 4820, 976, 29, 31, 19144, 86, 19145, 639, 1, 9, 1237, 25, 19, 63, 2666, 1281, 480, 9, 939, 13, 63, 26, 73, 225, 678, 241, 4, 6349, 9216, 12184, 2102, 886, 7431, 109, 603, 488, 1586, 5, 42, 7, 470, 1, 1636, 5, 42, 7, 763, 870, 49, 539, 84, 307, 7432, 39, 21, 51, 19146, 291, 1238, 32, 43, 12185, 7433, 14, 1237, 677, 75, 19147, 12, 763, 870, 75, 9, 3372, 307, 1407, 201, 9217, 3373, 167, 6, 3372, 12186, 8, 1873, 32, 43, 22, 1636, 317, 1014, 510, 16, 1635, 612, 210, 3, 341, 12182, 211, 2103, 2, 59, 3, 5454, 2667, 136, 10, 19148, 136, 698, 313, 3940, 584, 1, 247, 417, 9, 19149, 5455, 1, 1357, 86, 51, 1874, 12187, 8, 1406, 286, 445, 115, 500, 109, 1586, 49, 23, 7, 470, 12188, 8, 63, 29, 19150, 32, 7, 472, 13, 185, 19151, 24, 34, 1, 291, 5456, 8, 63, 167, 9, 9218, 32, 7, 472, 18, 8, 4, 9, 46, 211, 9219, 331, 19152, 2, 2396, 136, 12189, 584, 152, 155, 2103, 11, 291, 55, 60, 19153, 18, 60, 19154, 22, 19155, 145, 435, 676, 1179, 3374, 307, 3941, 4, 1236, 2, 8, 29, 2397, 32, 43, 12190, 280, 676, 19156, 4350, 1, 8, 29, 2397, 32, 43, 3158, 392, 27, 676, 3375, 4351, 49, 8, 4821, 584, 16, 165, 2535, 19157, 477, 94, 2668, 3376, 8, 11, 291, 231, 19158, 18, 9220, 1408, 19159, 5457, 3376, 8, 11, 291, 22, 4351, 8, 1637, 1406, 12191, 19160, 65, 18, 3376, 29, 1750, 11, 8, 19161, 1, 2666, 32, 43, 2, 122, 4352, 12190, 13, 8, 29, 1098, 266, 84, 7434, 640, 3631, 5, 12192, 310, 12193, 676, 2, 19162, 295, 185, 29, 63, 12194, 167, 539, 4, 621, 423, 570, 103, 1506, 470, 699, 1, 20, 301, 1635, 291, 12195, 295, 1, 19163, 166, 4, 5454, 2536, 93, 8, 718, 276, 1751, 242, 241, 4, 1282, 12, 281, 4, 1587, 12196, 12197, 12198, 1, 6350, 291, 418, 192, 13, 747, 20, 19164, 349, 192, 4, 291, 501, 167, 9, 1283, 310, 62, 3, 28, 47, 297, 104, 19, 398, 138, 40, 3377, 287, 12199, 2, 19165, 845, 1, 291, 24, 9221, 32, 43]\n",
      "879\n"
     ]
    }
   ],
   "source": [
    "train_sentences, test_sentences, y_train, y_test = train_test_split(train_data, train_labels, test_size = .1, stratify = train_labels)\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "x_train = tokenizer.texts_to_sequences(train_sentences)\n",
    "x_test = tokenizer.texts_to_sequences(test_sentences)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)\n",
    "print(train_sentences[0])\n",
    "print(x_train[0])\n",
    "maxLen = max([len(x_train[i]) for i in range(len(x_train))])\n",
    "print(maxLen)\n",
    "x_train = pad_sequences(x_train, padding = 'post', maxlen=maxLen)\n",
    "x_test = pad_sequences(x_test, padding = 'post', maxlen=maxLen)\n",
    "x_train = np.array(x_train, dtype=np.float)\n",
    "x_test = np.array(x_test, dtype=np.float)\n",
    "y_train = np.array(y_train, dtype=np.float)\n",
    "y_test = np.array(y_test, dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1cd274df-ca75-41a2-8841-caa691c1571b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_13 (Embedding)    (None, 879, 100)          4755600   \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 875, 128)          64128     \n",
      "                                                                 \n",
      " global_max_pooling1d_6 (Glo  (None, 128)              0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 50)                6450      \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 20)                1020      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,827,198\n",
      "Trainable params: 4,827,198\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Embedding(input_dim = vocab_size, output_dim = embedding_dim, input_length = maxLen))\n",
    "model.add(keras.layers.Conv1D(128, 5, activation='relu'))\n",
    "model.add(keras.layers.GlobalMaxPool1D())\n",
    "model.add(keras.layers.Dense(50, activation = 'relu'))\n",
    "model.add(keras.layers.Dense(20, activation = 'softmax'))\n",
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "11822901-ab27-4a0a-a79d-7ad4527b8b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "41/41 [==============================] - 4s 84ms/step - loss: 2.7907 - accuracy: 0.1424 - val_loss: 2.5797 - val_accuracy: 0.1944\n",
      "Epoch 2/30\n",
      "41/41 [==============================] - 3s 81ms/step - loss: 2.4067 - accuracy: 0.2283 - val_loss: 2.3455 - val_accuracy: 0.2639\n",
      "Epoch 3/30\n",
      "41/41 [==============================] - 3s 81ms/step - loss: 2.0071 - accuracy: 0.4141 - val_loss: 2.0697 - val_accuracy: 0.3750\n",
      "Epoch 4/30\n",
      "41/41 [==============================] - 4s 87ms/step - loss: 1.5277 - accuracy: 0.6246 - val_loss: 1.8459 - val_accuracy: 0.4722\n",
      "Epoch 5/30\n",
      "41/41 [==============================] - 4s 88ms/step - loss: 1.0010 - accuracy: 0.7802 - val_loss: 1.7512 - val_accuracy: 0.4931\n",
      "Epoch 6/30\n",
      "41/41 [==============================] - 4s 94ms/step - loss: 0.5648 - accuracy: 0.8893 - val_loss: 1.7094 - val_accuracy: 0.4861\n",
      "Epoch 7/30\n",
      "41/41 [==============================] - 4s 87ms/step - loss: 0.2920 - accuracy: 0.9628 - val_loss: 1.7109 - val_accuracy: 0.5000\n",
      "Epoch 8/30\n",
      "41/41 [==============================] - 4s 87ms/step - loss: 0.1522 - accuracy: 0.9861 - val_loss: 1.7293 - val_accuracy: 0.5139\n",
      "Epoch 9/30\n",
      "41/41 [==============================] - 4s 92ms/step - loss: 0.0810 - accuracy: 0.9969 - val_loss: 1.7472 - val_accuracy: 0.5069\n",
      "Epoch 10/30\n",
      "41/41 [==============================] - 4s 89ms/step - loss: 0.0480 - accuracy: 0.9969 - val_loss: 1.7475 - val_accuracy: 0.5139\n",
      "Epoch 11/30\n",
      "41/41 [==============================] - 4s 88ms/step - loss: 0.0366 - accuracy: 0.9977 - val_loss: 1.7947 - val_accuracy: 0.4931\n",
      "Epoch 12/30\n",
      "41/41 [==============================] - 4s 93ms/step - loss: 0.0230 - accuracy: 0.9985 - val_loss: 1.7848 - val_accuracy: 0.4931\n",
      "Epoch 13/30\n",
      "41/41 [==============================] - 4s 93ms/step - loss: 0.0170 - accuracy: 0.9985 - val_loss: 1.8009 - val_accuracy: 0.5069\n",
      "Epoch 14/30\n",
      "41/41 [==============================] - 4s 93ms/step - loss: 0.0123 - accuracy: 0.9992 - val_loss: 1.7947 - val_accuracy: 0.5000\n",
      "Epoch 15/30\n",
      "41/41 [==============================] - 4s 94ms/step - loss: 0.0099 - accuracy: 0.9992 - val_loss: 1.8010 - val_accuracy: 0.4931\n",
      "Epoch 16/30\n",
      "41/41 [==============================] - 4s 89ms/step - loss: 0.0084 - accuracy: 0.9992 - val_loss: 1.8158 - val_accuracy: 0.4931\n",
      "Epoch 17/30\n",
      "41/41 [==============================] - 4s 89ms/step - loss: 0.0068 - accuracy: 0.9992 - val_loss: 1.8133 - val_accuracy: 0.5000\n",
      "Epoch 18/30\n",
      "41/41 [==============================] - 4s 93ms/step - loss: 0.0062 - accuracy: 0.9992 - val_loss: 1.8241 - val_accuracy: 0.4931\n",
      "Epoch 19/30\n",
      "41/41 [==============================] - 4s 94ms/step - loss: 0.0054 - accuracy: 0.9992 - val_loss: 1.8244 - val_accuracy: 0.5139\n",
      "Epoch 20/30\n",
      "41/41 [==============================] - 4s 96ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 1.8325 - val_accuracy: 0.5139\n",
      "Epoch 21/30\n",
      "41/41 [==============================] - 4s 92ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.8393 - val_accuracy: 0.5139\n",
      "Epoch 22/30\n",
      "41/41 [==============================] - 4s 89ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 1.8428 - val_accuracy: 0.5139\n",
      "Epoch 23/30\n",
      "41/41 [==============================] - 4s 94ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 1.8490 - val_accuracy: 0.5139\n",
      "Epoch 24/30\n",
      "41/41 [==============================] - 4s 92ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 1.8540 - val_accuracy: 0.5139\n",
      "Epoch 25/30\n",
      "41/41 [==============================] - 4s 89ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 1.8583 - val_accuracy: 0.5139\n",
      "Epoch 26/30\n",
      "41/41 [==============================] - 4s 89ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 1.8632 - val_accuracy: 0.5139\n",
      "Epoch 27/30\n",
      "41/41 [==============================] - 4s 97ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 1.8697 - val_accuracy: 0.5139\n",
      "Epoch 28/30\n",
      "41/41 [==============================] - 4s 91ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 1.8757 - val_accuracy: 0.5139\n",
      "Epoch 29/30\n",
      "41/41 [==============================] - 4s 90ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 1.8747 - val_accuracy: 0.5208\n",
      "Epoch 30/30\n",
      "41/41 [==============================] - 4s 93ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 1.8815 - val_accuracy: 0.5069\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26e4847e460>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs = 30, validation_data = (x_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
